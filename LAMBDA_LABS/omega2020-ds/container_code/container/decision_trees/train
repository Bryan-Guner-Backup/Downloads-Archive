#!/usr/bin/env python3

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback
import boto3
from decouple import config
import pandas as pd
import numpy as np
#from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

import xgboost as xgb
# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')



# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.

channel_name='training'
training_path = os.path.join(input_path, channel_name)

# The function to execute the training.
def train():
    print('Starting the training.')
    try:

        #No Hyper Params Used Yet - so skipping the loading of them
        # # Read in any hyperparameters that the user passed with the training job
        # with open(param_path, 'r') as tc:
        #     trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        print("loading files")
        # print("Splitting files")
        # train_data, test_data = train_test_split(all_data, test_size=0.2)
        

        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        print(len(input_files))
        raw_data = [ pd.read_csv(file,header=None) for file in input_files ]
        train_all = pd.concat(raw_data)
        train_all = train_all = train_all.iloc[1:]

        train_data , validation_data = train_test_split(train_all, test_size=0.20)

        train_y = train_data[train_data.columns[0]]
        train_X = train_data[train_data.columns[1:]]
        validation_y = validation_data[validation_data.columns[0]]
        validation_X = validation_data[validation_data.columns[1:]]

        data_header = list(range(1, 785))

        train_X.columns = data_header
        validation_X.columns = data_header
        #train_y = pd.read_csv(str(training_path)+"/all_train.csv")
        #print("y split")
        #train_X = pd.read_csv(str(training_path)+"/train_X.csv")

        # Now use scikit-learn's decision tree classifier to train the model.
        print("running model")
        clf = xgb.XGBClassifier(objective="multi:softprob")
        clf.fit(train_X, train_y)
        print("model fit!")
        print('Validation Accuracy',clf.score(validation_X,validation_y))

        # save the model
        print("saving model")
        with open(os.path.join(model_path, 'xgboost-model.pkl'), 'wb') as out:
            pickle.dump(clf, out)
        print("model complete")
        print("starting validation test")
        y_pred = clf.predict(validation_X)
        print(classification_report(validation_y, y_pred))
        print("Exiting train script!")
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'wb') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
